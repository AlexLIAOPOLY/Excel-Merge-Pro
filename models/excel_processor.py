try:
    import pandas as pd
    HAS_PANDAS = True
except ImportError:
    HAS_PANDAS = False
    pd = None
import os
import time
import threading
from datetime import datetime
from difflib import SequenceMatcher
import hashlib
from functools import lru_cache
from contextlib import contextmanager
from models.database import db, TableData, TableSchema, UploadHistory, TableGroup, ColumnMapping

class UniversalExcelProcessor:
    """é€šç”¨Excelè¡¨æ ¼å¤„ç†å™¨ï¼Œæ”¯æŒä»»æ„æ ¼å¼çš„è¡¨æ ¼åˆå¹¶å’Œæ™ºèƒ½åˆ†ç»„"""
    
    # ç›¸ä¼¼åº¦é˜ˆå€¼é…ç½®
    EXACT_MATCH_THRESHOLD = 1.0      # å®Œå…¨åŒ¹é…
    HIGH_SIMILARITY_THRESHOLD = 0.95 # é«˜ç›¸ä¼¼åº¦ - æé«˜é˜ˆå€¼ä»¥ä¾¿æ›´å¥½åœ°åˆå¹¶ç›¸ä¼¼è¡¨æ ¼
    MIN_SIMILARITY_THRESHOLD = 0.85  # æœ€ä½ç›¸ä¼¼åº¦ - æé«˜é˜ˆå€¼é¿å…é”™è¯¯åˆå¹¶
    
    # å¹¶å‘å®‰å…¨æ§åˆ¶
    _creation_lock = threading.Lock()
    _fingerprint_cache = {}
    _similarity_cache = {}
    
    # è¿›åº¦è·Ÿè¸ª
    _current_progress = {'stage': '', 'percent': 0, 'message': ''}
    
    @classmethod
    def clear_cache(cls):
        """æ¸…ç†ç¼“å­˜"""
        cls._fingerprint_cache.clear()
        cls._similarity_cache.clear()
    
    @classmethod
    def get_progress(cls):
        """è·å–å½“å‰å¤„ç†è¿›åº¦"""
        return cls._current_progress.copy()
    
    @classmethod
    def _update_progress(cls, stage, percent, message):
        """æ›´æ–°å¤„ç†è¿›åº¦"""
        cls._current_progress.update({
            'stage': stage,
            'percent': percent,
            'message': message
        })
    
    @staticmethod
    @contextmanager
    def database_transaction():
        """å®‰å…¨çš„æ•°æ®åº“äº‹åŠ¡ç®¡ç†å™¨"""
        try:
            yield db.session
            db.session.commit()
        except Exception as e:
            db.session.rollback()
            raise e
    
    @staticmethod
    def detect_header_row(df, max_check_rows=5):
        """æ™ºèƒ½æ£€æµ‹è¡¨å¤´è¡Œä½ç½®"""
        print("[ç³»ç»Ÿ] å¼€å§‹æ£€æµ‹è¡¨å¤´ä½ç½®...")
        
        for i in range(min(max_check_rows, len(df))):
            row = df.iloc[i]
            # æ£€æŸ¥è¿™ä¸€è¡Œæ˜¯å¦åƒè¡¨å¤´ï¼ˆéç©ºå€¼è¾ƒå¤šï¼Œä¸”åŒ…å«æ–‡å­—ï¼‰
            non_null_count = row.count()
            text_count = sum(1 for val in row if isinstance(val, str) and len(str(val).strip()) > 0)
            
            # å¦‚æœéç©ºå€¼è¾ƒå¤šä¸”å¤§éƒ¨åˆ†æ˜¯æ–‡å­—ï¼Œå¯èƒ½æ˜¯è¡¨å¤´
            if non_null_count >= len(row) * 0.5 and text_count >= non_null_count * 0.7:
                print(f"[ç³»ç»Ÿ] æ£€æµ‹åˆ°è¡¨å¤´å¯èƒ½åœ¨ç¬¬ {i+1} è¡Œ")
                return i
        
        print("[ç³»ç»Ÿ] æœªæ£€æµ‹åˆ°æ˜æ˜¾è¡¨å¤´ï¼Œä½¿ç”¨ç¬¬1è¡Œä½œä¸ºè¡¨å¤´")
        return 0
    
    @staticmethod
    def clean_column_names(columns):
        """æ¸…ç†åˆ—å - å¢å¼ºç‰ˆæœ¬ï¼Œæ”¯æŒæ›´æ™ºèƒ½çš„æ ‡å‡†åŒ–å’Œé‡å¤åˆ—åå¤„ç†"""
        import re
        import unicodedata
        cleaned = []
        seen_names = {}  # è·Ÿè¸ªå·²ä½¿ç”¨çš„åˆ—å
        
        for i, col in enumerate(columns):
            if pd.isna(col):
                col_str = f"æœªå‘½ååˆ—_{i+1}"
            else:
                col_str = str(col)
                
                # 1. Unicodeæ ‡å‡†åŒ–ï¼ˆå¤„ç†å„ç§ç©ºæ ¼å­—ç¬¦ï¼‰
                col_str = unicodedata.normalize('NFKC', col_str)
                
                # 2. å»é™¤é¦–å°¾ç©ºæ ¼
                col_str = col_str.strip()
                
                # 3. å°†å„ç§ç©ºç™½å­—ç¬¦ï¼ˆåŒ…æ‹¬ä¸­æ–‡å…¨è§’ç©ºæ ¼ã€åˆ¶è¡¨ç¬¦ç­‰ï¼‰æ›¿æ¢ä¸ºå•ä¸ªåŠè§’ç©ºæ ¼
                col_str = re.sub(r'[\s\u00A0\u2000-\u200B\u2028\u2029\u3000]+', ' ', col_str)
                
                # 4. å»é™¤ç‰¹æ®Šçš„ä¸å¯è§å­—ç¬¦
                col_str = re.sub(r'[\u200E\u200F\uFEFF]', '', col_str)
                
                # 5. å†æ¬¡å»é™¤é¦–å°¾ç©ºæ ¼
                col_str = col_str.strip()
                
                # 6. å¦‚æœæ¸…ç†åä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤åç§°
                if not col_str:
                    col_str = f"æœªå‘½ååˆ—_{i+1}"
            
            # 7. å¤„ç†é‡å¤åˆ—å
            original_name = col_str
            counter = 1
            while col_str in seen_names:
                col_str = f"{original_name}_{counter}"
                counter += 1
                if counter > 100:  # é˜²æ­¢æ— é™å¾ªç¯
                    col_str = f"{original_name}_{i+1}"
                    break
            
            seen_names[col_str] = True
            cleaned.append(col_str)
        
        return cleaned
    
    @staticmethod
    def update_table_schema(columns):
        """æ›´æ–°è¡¨æ ¼ç»“æ„"""
        print(f"[ç³»ç»Ÿ] æ›´æ–°è¡¨æ ¼ç»“æ„ï¼Œå…± {len(columns)} åˆ—")
        
        for i, col_name in enumerate(columns):
            # æ£€æŸ¥åˆ—æ˜¯å¦å·²å­˜åœ¨
            existing = TableSchema.query.filter_by(column_name=col_name).first()
            if not existing:
                schema = TableSchema(
                    column_name=col_name,
                    column_type='text',
                    column_order=i,
                    is_active=True
                )
                db.session.add(schema)
                print(f"[ç³»ç»Ÿ] æ·»åŠ æ–°åˆ—: {col_name}")
        
        try:
            db.session.commit()
        except Exception as e:
            print(f"[é”™è¯¯] æ›´æ–°è¡¨æ ¼ç»“æ„æ—¶å‡ºé”™: {str(e)}")
            db.session.rollback()
    
    @staticmethod
    def get_current_schema():
        """è·å–å½“å‰è¡¨æ ¼ç»“æ„"""
        schema = TableSchema.query.filter_by(is_active=True).order_by(TableSchema.column_order).all()
        return [s.column_name for s in schema]
    
    @staticmethod
    def process_excel_file(file_path, filename):
        """å¤„ç†Excelæ–‡ä»¶ï¼Œæ”¯æŒä»»æ„æ ¼å¼"""
        print(f"[ç³»ç»Ÿ] å¼€å§‹å¤„ç†Excelæ–‡ä»¶: {filename}")
        
        try:
            # ç»Ÿä¸€ä½¿ç”¨openpyxlå¤„ç†æ‰€æœ‰Excelæ–‡ä»¶
            df_raw = pd.read_excel(file_path, engine='openpyxl', header=None)
            
            if df_raw.empty:
                return False, "æ–‡ä»¶ä¸ºç©º", 0
            
            print(f"[ç³»ç»Ÿ] æ–‡ä»¶è¯»å–æˆåŠŸï¼ŒåŸå§‹æ•°æ® {len(df_raw)} è¡Œ x {len(df_raw.columns)} åˆ—")
            
            # æ™ºèƒ½æ£€æµ‹è¡¨å¤´ä½ç½®
            header_row = UniversalExcelProcessor.detect_header_row(df_raw)
            
            # é‡æ–°è¯»å–æ–‡ä»¶ï¼Œä½¿ç”¨æ£€æµ‹åˆ°çš„è¡¨å¤´
            df = pd.read_excel(file_path, engine='openpyxl', header=header_row)
            
            # æ¸…ç†æ•°æ®
            df = df.dropna(how='all')  # åˆ é™¤å…¨ç©ºè¡Œ
            df = df.dropna(axis=1, how='all')  # åˆ é™¤å…¨ç©ºåˆ—
            
            if df.empty:
                return False, "æ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆæ•°æ®", 0
            
            # æ¸…ç†åˆ—å
            df.columns = UniversalExcelProcessor.clean_column_names(df.columns)
            
            print(f"[ç³»ç»Ÿ] å¤„ç†åæ•°æ®: {len(df)} è¡Œ x {len(df.columns)} åˆ—")
            print(f"[ç³»ç»Ÿ] æ£€æµ‹åˆ°çš„åˆ—å: {list(df.columns)}")
            
            # æ›´æ–°è¡¨æ ¼ç»“æ„
            UniversalExcelProcessor.update_table_schema(df.columns)
            
            # å¯¼å…¥æ•°æ®
            imported_count = 0
            for index, row in df.iterrows():
                # è·³è¿‡å…¨ç©ºè¡Œ
                if row.isna().all():
                    continue
                
                # åˆ›å»ºæ•°æ®å­—å…¸
                row_dict = {}
                for col_name in df.columns:
                    value = row[col_name]
                    if pd.notna(value):
                        row_dict[col_name] = str(value)
                    else:
                        row_dict[col_name] = ""
                
                # å¦‚æœæ•´è¡Œéƒ½æ˜¯ç©ºçš„ï¼Œè·³è¿‡
                if not any(v.strip() for v in row_dict.values() if v):
                    continue
                
                # åˆ›å»ºæ•°æ®è®°å½•
                table_data = TableData()
                table_data.source_file = filename
                table_data.set_data(row_dict)
                
                try:
                    db.session.add(table_data)
                    imported_count += 1
                except Exception as e:
                    print(f"[é”™è¯¯] å¯¼å…¥ç¬¬ {index+1} è¡Œæ•°æ®æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            db.session.commit()
            print(f"[ç³»ç»Ÿ] æˆåŠŸå¯¼å…¥ {imported_count} æ¡æ•°æ®")
            
            # è®°å½•ä¸Šä¼ å†å²
            history = UploadHistory(
                filename=filename,
                rows_imported=imported_count,
                status='success'
            )
            history.set_columns(list(df.columns))
            db.session.add(history)
            db.session.commit()
            
            return True, "å¯¼å…¥æˆåŠŸ", imported_count
            
        except Exception as e:
            print(f"[é”™è¯¯] å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            db.session.rollback()
            
            # è®°å½•é”™è¯¯å†å²
            history = UploadHistory(
                filename=filename,
                rows_imported=0,
                status='failed',
                error_message=str(e)
            )
            db.session.add(history)
            db.session.commit()
            
            return False, str(e), 0
    
    @staticmethod
    def get_all_data():
        """è·å–æ‰€æœ‰æ•°æ®ï¼ŒæŒ‰åˆ›å»ºæ—¶é—´æ’åº"""
        data = TableData.query.order_by(TableData.created_at.asc()).all()
        return [item.to_dict() for item in data]
    
    @staticmethod
    def get_data_stats():
        """è·å–æ•°æ®ç»Ÿè®¡"""
        total_count = TableData.query.count()
        
        # ç»Ÿè®¡æ¥æºæ–‡ä»¶æ•° - æ”¹è¿›é€»è¾‘ï¼Œæ’é™¤ç©ºå€¼å’Œåªæœ‰æ‰©å±•åçš„æƒ…å†µ
        source_files = db.session.query(TableData.source_file).distinct().all()
        valid_files = []
        for f in source_files:
            filename = f[0]
            if filename and filename.strip():
                # æ’é™¤åªæœ‰æ‰©å±•åçš„æƒ…å†µï¼ˆå¦‚"xlsx", "xls"ç­‰ï¼‰
                if '.' in filename and len(filename.split('.')[0]) > 0:
                    valid_files.append(filename)
                elif filename not in ['xlsx', 'xls', 'æ‰‹åŠ¨æ·»åŠ ']:
                    valid_files.append(filename)
        
        file_count = len(valid_files)
        
        # ç»Ÿè®¡åˆ—æ•° - ä¼˜å…ˆä½¿ç”¨åˆ†ç»„æ•°æ®
        schema_count = 0
        groups = TableGroup.query.all()
        if groups:
            # ä½¿ç”¨æœ€æ–°åˆ†ç»„çš„åˆ—æ•°
            latest_group = groups[-1]
            schema_count = latest_group.column_count
        else:
            # å…¼å®¹æ—§æ•°æ®
            schema_count = TableSchema.query.filter_by(is_active=True).count()
        
        return {
            'total_records': total_count,
            'source_files': file_count,
            'total_columns': schema_count,
            'last_update': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
    
    @staticmethod
    def update_record(record_id, data):
        """æ›´æ–°è®°å½•"""
        record = TableData.query.get(record_id)
        if record:
            current_data = record.get_data()
            current_data.update(data)
            record.set_data(current_data)
            record.updated_at = datetime.utcnow()
            db.session.commit()
            return True
        return False
    
    @staticmethod
    def delete_record(record_id):
        """åˆ é™¤è®°å½•"""
        record = TableData.query.get(record_id)
        if record:
            db.session.delete(record)
            db.session.commit()
            return True
        return False
    
    @staticmethod
    def clear_all_data():
        """æ¸…ç©ºæ‰€æœ‰æ•°æ®"""
        TableData.query.delete()
        TableSchema.query.delete()
        db.session.commit()
    
    @staticmethod
    def add_column(column_name, insert_position=None):
        """æ·»åŠ æ–°åˆ—ï¼Œæ”¯æŒæŒ‡å®šä½ç½®æ’å…¥"""
        existing = TableSchema.query.filter_by(column_name=column_name).first()
        if existing:
            return False, "åˆ—åå·²å­˜åœ¨"
        
        try:
            if insert_position is None:
                # æ·»åŠ åˆ°æœ«å°¾
                max_order = db.session.query(db.func.max(TableSchema.column_order)).scalar() or 0
                new_order = max_order + 1
            else:
                # åœ¨æŒ‡å®šä½ç½®æ’å…¥ï¼Œéœ€è¦è°ƒæ•´åç»­åˆ—çš„é¡ºåº
                new_order = insert_position
                
                # å°†æŒ‡å®šä½ç½®åŠä¹‹åçš„åˆ—çš„orderéƒ½åŠ 1
                columns_to_update = TableSchema.query.filter(
                    TableSchema.column_order >= insert_position,
                    TableSchema.is_active == True
                ).all()
                
                for col in columns_to_update:
                    col.column_order += 1
            
            # åˆ›å»ºæ–°åˆ—
            new_column = TableSchema(
                column_name=column_name,
                column_type='text',
                column_order=new_order,
                is_active=True
            )
            
            db.session.add(new_column)
            db.session.commit()
            print(f"[ç³»ç»Ÿ] æˆåŠŸæ·»åŠ åˆ—: {column_name} åœ¨ä½ç½® {new_order}")
            return True, "åˆ—æ·»åŠ æˆåŠŸ"
        except Exception as e:
            db.session.rollback()
            print(f"[é”™è¯¯] æ·»åŠ åˆ—æ—¶å‡ºé”™: {str(e)}")
            return False, str(e)
    
    @staticmethod
    def delete_column(column_name):
        """åˆ é™¤åˆ—"""
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€åä¸€åˆ—
        active_columns = TableSchema.query.filter_by(is_active=True).count()
        if active_columns <= 1:
            return False, "ä¸èƒ½åˆ é™¤æœ€åä¸€åˆ—"
        
        # æ‰¾åˆ°è¦åˆ é™¤çš„åˆ—
        column = TableSchema.query.filter_by(column_name=column_name, is_active=True).first()
        if not column:
            return False, "åˆ—ä¸å­˜åœ¨"
        
        try:
            # è½¯åˆ é™¤ï¼šè®¾ç½®ä¸ºéæ´»è·ƒçŠ¶æ€
            column.is_active = False
            
            # ä»æ‰€æœ‰æ•°æ®è®°å½•ä¸­åˆ é™¤è¯¥åˆ—çš„æ•°æ®
            all_records = TableData.query.all()
            for record in all_records:
                data = record.get_data()
                if column_name in data:
                    del data[column_name]
                    record.set_data(data)
            
            db.session.commit()
            print(f"[ç³»ç»Ÿ] æˆåŠŸåˆ é™¤åˆ—: {column_name}")
            return True, "åˆ—åˆ é™¤æˆåŠŸ"
        except Exception as e:
            db.session.rollback()
            print(f"[é”™è¯¯] åˆ é™¤åˆ—æ—¶å‡ºé”™: {str(e)}")
            return False, str(e)
    
    @staticmethod
    def rename_column(old_name, new_name):
        """é‡å‘½ååˆ—"""
        # æ£€æŸ¥æ—§åˆ—æ˜¯å¦å­˜åœ¨
        old_column = TableSchema.query.filter_by(column_name=old_name, is_active=True).first()
        if not old_column:
            return False, "åŸåˆ—åä¸å­˜åœ¨"
        
        # æ£€æŸ¥æ–°åˆ—åæ˜¯å¦å·²å­˜åœ¨
        existing = TableSchema.query.filter_by(column_name=new_name, is_active=True).first()
        if existing:
            return False, "æ–°åˆ—åå·²å­˜åœ¨"
        
        try:
            # æ›´æ–°åˆ—ç»“æ„
            old_column.column_name = new_name
            
            # æ›´æ–°æ‰€æœ‰æ•°æ®è®°å½•ä¸­çš„å­—æ®µå
            all_records = TableData.query.all()
            for record in all_records:
                data = record.get_data()
                if old_name in data:
                    data[new_name] = data.pop(old_name)
                    record.set_data(data)
            
            db.session.commit()
            print(f"[ç³»ç»Ÿ] æˆåŠŸé‡å‘½ååˆ—: {old_name} -> {new_name}")
            return True, "åˆ—é‡å‘½åæˆåŠŸ"
        except Exception as e:
            db.session.rollback()
            print(f"[é”™è¯¯] é‡å‘½ååˆ—æ—¶å‡ºé”™: {str(e)}")
            return False, str(e)
    
    @staticmethod
    def add_row(insert_after_id=None, row_data=None):
        """æ·»åŠ æ–°è¡Œï¼Œæ”¯æŒæŒ‡å®šä½ç½®æ’å…¥"""
        try:
            # è·å–å½“å‰è¡¨æ ¼ç»“æ„
            schema = UniversalExcelProcessor.get_current_schema()
            
            if not schema:
                return False, "æ²¡æœ‰å¯ç”¨çš„è¡¨æ ¼ç»“æ„", None
            
            # åˆ›å»ºè¡Œæ•°æ®
            if row_data is None:
                row_data = {col: '' for col in schema}
            
            # åˆ›å»ºæ–°çš„æ•°æ®è®°å½•
            table_data = TableData()
            table_data.source_file = 'æ‰‹åŠ¨æ·»åŠ '
            table_data.set_data(row_data)
            
            # æäº¤åˆ°æ•°æ®åº“
            db.session.add(table_data)
            db.session.commit()
            
            # å¦‚æœæŒ‡å®šäº†æ’å…¥ä½ç½®ï¼Œæˆ‘ä»¬é€šè¿‡è®¾ç½®åˆ›å»ºæ—¶é—´æ¥å½±å“æ’åº
            if insert_after_id:
                # è·å–å‚è€ƒè¡Œçš„æ—¶é—´
                ref_row = TableData.query.get(insert_after_id)
                if ref_row:
                    # è®¾ç½®æ–°è¡Œçš„åˆ›å»ºæ—¶é—´ç¨æ™šäºå‚è€ƒè¡Œï¼Œä½†æ—©äºåç»­è¡Œ
                    from datetime import datetime, timedelta
                    table_data.created_at = ref_row.created_at + timedelta(microseconds=1)
                    db.session.commit()
            
            print(f"[ç³»ç»Ÿ] æˆåŠŸæ·»åŠ æ–°è¡Œï¼ŒID: {table_data.id}, æ’å…¥åœ¨ID {insert_after_id} ä¹‹å")
            return True, "æ–°è¡Œæ·»åŠ æˆåŠŸ", table_data.id
            
        except Exception as e:
            db.session.rollback()
            print(f"[é”™è¯¯] æ·»åŠ è¡Œæ—¶å‡ºé”™: {str(e)}")
            return False, str(e), None
    
    # ==================== æ™ºèƒ½è¡¨æ ¼åˆ†ç»„åŠŸèƒ½ ====================
    
    @classmethod
    def calculate_column_similarity(cls, columns1, columns2):
        """
        è®¡ç®—ä¸¤ä¸ªåˆ—ååˆ—è¡¨çš„ç›¸ä¼¼åº¦ - æ™ºèƒ½ç‰ˆæœ¬ï¼Œå¸¦ç¼“å­˜ä¼˜åŒ–
        æ”¯æŒåˆ—é¡ºåºä¸åŒã€ç©ºæ ¼å·®å¼‚ã€å¤§å°å†™å·®å¼‚çš„æƒ…å†µ
        """
        if not columns1 or not columns2:
            return 0.0
        
        # åˆ›å»ºç¼“å­˜é”®ï¼ˆç¡®ä¿é¡ºåºæ— å…³ï¼‰
        key1 = '|'.join(sorted(str(col) for col in columns1))
        key2 = '|'.join(sorted(str(col) for col in columns2))
        cache_key = f"{key1}##{key2}" if key1 <= key2 else f"{key2}##{key1}"
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_key in cls._similarity_cache:
            return cls._similarity_cache[cache_key]
        
        # é¦–å…ˆæ¸…ç†å’Œæ ‡å‡†åŒ–åˆ—å
        cleaned_cols1 = [cls._normalize_column_name(col) for col in columns1]
        cleaned_cols2 = [cls._normalize_column_name(col) for col in columns2]
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        if len(cleaned_cols1) != len(cleaned_cols2):
            # å¦‚æœåˆ—æ•°ä¸åŒï¼Œä½¿ç”¨é›†åˆç›¸ä¼¼åº¦ + é•¿åº¦æƒ©ç½š
            set1 = set(cleaned_cols1)
            set2 = set(cleaned_cols2)
            
            # è®¡ç®—Jaccardç›¸ä¼¼åº¦
            intersection = len(set1.intersection(set2))
            union = len(set1.union(set2))
            jaccard_similarity = intersection / union if union > 0 else 0.0
            
            # é•¿åº¦å·®å¼‚æƒ©ç½š
            length_ratio = min(len(cleaned_cols1), len(cleaned_cols2)) / max(len(cleaned_cols1), len(cleaned_cols2))
            
            similarity = jaccard_similarity * length_ratio
        else:
            # åˆ—æ•°ç›¸åŒæ—¶ï¼Œä½¿ç”¨æœ€ä¼˜åŒ¹é…ç®—æ³•
            similarity = cls._calculate_optimal_column_matching(cleaned_cols1, cleaned_cols2)
        
        # ç¼“å­˜ç»“æœï¼ˆé™åˆ¶ç¼“å­˜å¤§å°ï¼‰
        if len(cls._similarity_cache) < 1000:
            cls._similarity_cache[cache_key] = similarity
        
        return similarity
    
    @staticmethod
    def _normalize_column_name(column_name):
        """æ ‡å‡†åŒ–å•ä¸ªåˆ—åç”¨äºæ¯”è¾ƒï¼ŒåŒ…å«æ™ºèƒ½åˆ«åè¯†åˆ«"""
        if pd.isna(column_name):
            return ""
        
        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶è½¬å°å†™
        normalized = str(column_name).lower()
        
        # å»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦å’Œç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™å­—æ¯æ•°å­—å’Œä¸­æ–‡å­—ç¬¦
        import re
        normalized = re.sub(r'[^\w\u4e00-\u9fff]', '', normalized)
        
        # æ™ºèƒ½åˆ«åè¯†åˆ« - å°†å¸¸è§çš„åŒä¹‰è¯æ ‡å‡†åŒ–
        column_aliases = {
            # IDç›¸å…³
            'ç¼–å·': 'id', 'number': 'id', 'num': 'id', 'åºå·': 'id', 'id': 'id',
            # åç§°ç›¸å…³
            'åç§°': 'name', 'name': 'name', 'å§“å': 'name', 'åå­—': 'name',
            # æ—¥æœŸç›¸å…³
            'æ—¥æœŸ': 'date', 'date': 'date', 'æ—¶é—´': 'date', 'time': 'date',
            'åˆ›å»ºæ—¥æœŸ': 'createdate', 'åˆ›å»ºæ—¶é—´': 'createdate',
            'æ›´æ–°æ—¥æœŸ': 'updatedate', 'æ›´æ–°æ—¶é—´': 'updatedate',
            # çŠ¶æ€ç›¸å…³
            'çŠ¶æ€': 'status', 'status': 'status', 'æƒ…å†µ': 'status',
            # å¤‡æ³¨ç›¸å…³
            'å¤‡æ³¨': 'remark', 'remark': 'remark', 'è¯´æ˜': 'remark', 'description': 'remark',
            # é‡‘é¢ç›¸å…³
            'é‡‘é¢': 'amount', 'amount': 'amount', 'ä»·æ ¼': 'amount', 'price': 'amount',
            # æ•°é‡ç›¸å…³
            'æ•°é‡': 'quantity', 'quantity': 'quantity', 'ä¸ªæ•°': 'quantity', 'æ•°ç›®': 'quantity',
            # éƒ¨é—¨ç›¸å…³
            'éƒ¨é—¨': 'department', 'department': 'department', 'dept': 'department',
            # è”ç³»æ–¹å¼
            'ç”µè¯': 'phone', 'phone': 'phone', 'æ‰‹æœº': 'phone', 'mobile': 'phone',
            'é‚®ç®±': 'email', 'email': 'email', 'é‚®ä»¶': 'email', 'mail': 'email',
            # åœ°å€ç›¸å…³
            'åœ°å€': 'address', 'address': 'address', 'ä½ç½®': 'address', 'location': 'address',
        }
        
        # åº”ç”¨åˆ«åæ˜ å°„
        for alias, standard in column_aliases.items():
            if alias in normalized:
                normalized = normalized.replace(alias, standard)
        
        return normalized
    
    @staticmethod
    def _calculate_optimal_column_matching(cols1, cols2):
        """
        è®¡ç®—ä¸¤ä¸ªç­‰é•¿åˆ—è¡¨çš„æœ€ä¼˜åŒ¹é…ç›¸ä¼¼åº¦
        ä½¿ç”¨åŒˆç‰™åˆ©ç®—æ³•æ€æƒ³ï¼Œæ‰¾åˆ°æœ€ä½³çš„åˆ—å¯¹åº”å…³ç³»
        """
        n = len(cols1)
        if n == 0:
            return 1.0
        
        # æ„å»ºç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = []
        for col1 in cols1:
            row = []
            for col2 in cols2:
                # è®¡ç®—å•ä¸ªåˆ—åçš„ç›¸ä¼¼åº¦
                if col1 == col2:
                    # å®Œå…¨åŒ¹é…
                    similarity = 1.0
                elif col1 and col2:
                    # ä½¿ç”¨å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ç®—æ³•
                    similarity = SequenceMatcher(None, col1, col2).ratio()
                else:
                    similarity = 0.0
                row.append(similarity)
            similarity_matrix.append(row)
        
        # ä½¿ç”¨ç®€åŒ–çš„æœ€ä¼˜åŒ¹é…ç®—æ³•
        if n <= 10:  # å¯¹äºå°è§„æ¨¡ä½¿ç”¨ç²¾ç¡®ç®—æ³•
            return UniversalExcelProcessor._find_best_matching(similarity_matrix)
        else:  # å¯¹äºå¤§è§„æ¨¡ä½¿ç”¨è´ªå¿ƒç®—æ³•
            return UniversalExcelProcessor._greedy_matching(similarity_matrix)
    
    @staticmethod
    def _find_best_matching(similarity_matrix):
        """æ‰¾åˆ°æœ€ä½³åŒ¹é…ï¼ˆé€‚ç”¨äºå°è§„æ¨¡ï¼‰"""
        import itertools
        n = len(similarity_matrix)
        best_score = 0.0
        
        # å°è¯•æ‰€æœ‰å¯èƒ½çš„æ’åˆ—
        for perm in itertools.permutations(range(n)):
            score = sum(similarity_matrix[i][perm[i]] for i in range(n)) / n
            best_score = max(best_score, score)
        
        return best_score
    
    @staticmethod
    def _greedy_matching(similarity_matrix):
        """è´ªå¿ƒåŒ¹é…ç®—æ³•ï¼ˆé€‚ç”¨äºå¤§è§„æ¨¡ï¼‰"""
        n = len(similarity_matrix)
        used_cols = set()
        total_similarity = 0.0
        
        for i in range(n):
            best_match = -1
            best_sim = 0.0
            
            for j in range(n):
                if j not in used_cols and similarity_matrix[i][j] > best_sim:
                    best_sim = similarity_matrix[i][j]
                    best_match = j
            
            if best_match != -1:
                used_cols.add(best_match)
                total_similarity += best_sim
        
        return total_similarity / n
    
    @classmethod
    def generate_schema_fingerprint(cls, columns):
        """
        ç”Ÿæˆè¡¨å¤´æŒ‡çº¹ç”¨äºå¿«é€ŸåŒ¹é… - å¢å¼ºç‰ˆæœ¬ï¼Œå¸¦ç¼“å­˜ä¼˜åŒ–
        ä½¿ç”¨æ›´æ™ºèƒ½çš„æ ‡å‡†åŒ–æ–¹æ³•ï¼Œç¡®ä¿åˆ—é¡ºåºä¸åŒã€ç©ºæ ¼å·®å¼‚ç­‰æƒ…å†µä¸‹æŒ‡çº¹ä¸€è‡´
        """
        if not columns:
            return hashlib.md5(b'').hexdigest()
        
        # åˆ›å»ºç¼“å­˜é”®
        cache_key = '|'.join(str(col) for col in columns)
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_key in cls._fingerprint_cache:
            return cls._fingerprint_cache[cache_key]
        
        # ä½¿ç”¨æ–°çš„æ ‡å‡†åŒ–æ–¹æ³•ï¼Œç¡®ä¿ç©ºæ ¼ã€å¤§å°å†™ã€ç‰¹æ®Šå­—ç¬¦å¤„ç†ä¸€è‡´
        normalized_columns = []
        for col in columns:
            normalized = cls._normalize_column_name(col)
            if normalized:  # åªæ·»åŠ éç©ºçš„æ ‡å‡†åŒ–åˆ—å
                normalized_columns.append(normalized)
        
        # æ’åºç¡®ä¿é¡ºåºæ— å…³
        normalized_columns.sort()
        
        # ç”ŸæˆæŒ‡çº¹
        fingerprint_text = '|'.join(normalized_columns)
        fingerprint = hashlib.md5(fingerprint_text.encode('utf-8')).hexdigest()
        
        # ç¼“å­˜ç»“æœï¼ˆé™åˆ¶ç¼“å­˜å¤§å°ï¼‰
        if len(cls._fingerprint_cache) < 1000:
            cls._fingerprint_cache[cache_key] = fingerprint
        
        return fingerprint
    
    @classmethod
    def find_matching_table_group(cls, columns):
        """æŸ¥æ‰¾åŒ¹é…çš„è¡¨æ ¼åˆ†ç»„ - å¼ºåŒ–ç‰ˆæœ¬ï¼Œç»å¯¹é¿å…é‡å¤åˆ†ç»„"""
        print(f"[ç³»ç»Ÿ] æŸ¥æ‰¾åŒ¹é…çš„è¡¨æ ¼åˆ†ç»„ï¼Œåˆ—æ•°: {len(columns)}")
        print(f"[ç³»ç»Ÿ] è¾“å…¥åˆ—ç»“æ„: {columns}")
        
        # é¦–å…ˆæ¸…ç†è¾“å…¥çš„åˆ—å
        cleaned_columns = cls.clean_column_names(columns)
        print(f"[ç³»ç»Ÿ] æ¸…ç†ååˆ—ç»“æ„: {cleaned_columns}")
        
        # ç”Ÿæˆå½“å‰è¡¨å¤´çš„æŒ‡çº¹
        current_fingerprint = cls.generate_schema_fingerprint(cleaned_columns)
        print(f"[ç³»ç»Ÿ] ç”Ÿæˆçš„æŒ‡çº¹: {current_fingerprint}")
        
        # é¦–å…ˆæŸ¥æ‰¾å®Œå…¨åŒ¹é…çš„åˆ†ç»„ - ä½¿ç”¨æŒ‡çº¹å’Œåˆ—æ•°åŒé‡éªŒè¯
        exact_matches = TableGroup.query.filter_by(
            schema_fingerprint=current_fingerprint,
            column_count=len(cleaned_columns)
        ).all()
        
        if exact_matches:
            if len(exact_matches) > 1:
                print(f"[è­¦å‘Š] å‘ç° {len(exact_matches)} ä¸ªæŒ‡çº¹é‡å¤çš„åˆ†ç»„ï¼éœ€è¦åˆå¹¶")
                # å¦‚æœæœ‰å¤šä¸ªå®Œå…¨åŒ¹é…çš„åˆ†ç»„ï¼Œåˆå¹¶å®ƒä»¬
                main_group = exact_matches[0]
                for i in range(1, len(exact_matches)):
                    duplicate_group = exact_matches[i]
                    print(f"[ç³»ç»Ÿ] åˆå¹¶é‡å¤åˆ†ç»„ {duplicate_group.id} åˆ° {main_group.id}")
                    
                    # è¿ç§»æ•°æ®
                    data_records = TableData.query.filter_by(table_group_id=duplicate_group.id).all()
                    for record in data_records:
                        record.table_group_id = main_group.id
                    
                    # åˆ é™¤é‡å¤åˆ†ç»„
                    db.session.delete(duplicate_group)
                
                db.session.commit()
                print(f"[ç³»ç»Ÿ] é‡å¤åˆ†ç»„åˆå¹¶å®Œæˆï¼Œä½¿ç”¨åˆ†ç»„: {main_group.group_name}")
                return main_group, cls.EXACT_MATCH_THRESHOLD
            else:
                exact_match = exact_matches[0]
                print(f"[ç³»ç»Ÿ] æ‰¾åˆ°å®Œå…¨åŒ¹é…çš„åˆ†ç»„: {exact_match.group_name}")
                return exact_match, cls.EXACT_MATCH_THRESHOLD
        
        # æŸ¥æ‰¾ç›¸ä¼¼çš„åˆ†ç»„ - ä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨joinå‡å°‘æ•°æ®åº“æŸ¥è¯¢æ¬¡æ•°
        groups_with_schemas = db.session.query(TableGroup, TableSchema).join(
            TableSchema, TableGroup.id == TableSchema.table_group_id
        ).filter(
            TableGroup.column_count == len(cleaned_columns),
            TableSchema.is_active == True
        ).order_by(TableGroup.id, TableSchema.column_order).all()
        
        # æŒ‰åˆ†ç»„IDèšåˆåˆ—å
        group_columns_map = {}
        for group, schema in groups_with_schemas:
            if group.id not in group_columns_map:
                group_columns_map[group.id] = {'group': group, 'columns': []}
            group_columns_map[group.id]['columns'].append(schema.column_name)
        
        best_match = None
        best_similarity = 0.0
        
        for group_id, group_data in group_columns_map.items():
            group = group_data['group']
            group_columns = group_data['columns']
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarity = cls.calculate_column_similarity(cleaned_columns, group_columns)
            
            if similarity > best_similarity and similarity >= cls.MIN_SIMILARITY_THRESHOLD:
                best_match = group
                best_similarity = similarity
        
        if best_match:
            print(f"[ç³»ç»Ÿ] æ‰¾åˆ°ç›¸ä¼¼åˆ†ç»„: {best_match.group_name}, ç›¸ä¼¼åº¦: {best_similarity:.2f}")
            return best_match, best_similarity
        
        print("[ç³»ç»Ÿ] æœªæ‰¾åˆ°åŒ¹é…çš„åˆ†ç»„ï¼Œå°†åˆ›å»ºæ–°åˆ†ç»„")
        return None, 0.0
    
    @classmethod
    def create_table_group(cls, columns, filename):
        """åˆ›å»ºæ–°çš„è¡¨æ ¼åˆ†ç»„ - å¹¶å‘å®‰å…¨ç‰ˆæœ¬ï¼Œç»å¯¹é¿å…é‡å¤åˆ›å»º"""
        
        # ä½¿ç”¨é”ç¡®ä¿å¹¶å‘å®‰å…¨
        with cls._creation_lock:
            # é¦–å…ˆæ¸…ç†åˆ—å
            cleaned_columns = cls.clean_column_names(columns)
            
            # å†æ¬¡æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨åŒ¹é…çš„åˆ†ç»„ï¼ˆåŒé‡ä¿é™©ï¼‰
            existing_group, similarity = cls.find_matching_table_group(cleaned_columns)
            if existing_group and similarity >= cls.HIGH_SIMILARITY_THRESHOLD:
                print(f"[ç³»ç»Ÿ] åœ¨åˆ›å»ºåˆ†ç»„å‰å‘ç°åŒ¹é…åˆ†ç»„: {existing_group.group_name}ï¼Œç›´æ¥ä½¿ç”¨")
                return existing_group
            
            # ç”ŸæˆæŒ‡çº¹ï¼Œå†æ¬¡æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç›¸åŒæŒ‡çº¹çš„åˆ†ç»„
            new_fingerprint = cls.generate_schema_fingerprint(cleaned_columns)
            
            # ä½¿ç”¨äº‹åŠ¡ç¡®ä¿åŸå­æ€§
            with cls.database_transaction() as session:
                # å†æ¬¡æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒæŒ‡çº¹çš„åˆ†ç»„ï¼ˆé˜²æ­¢å¹¶å‘åˆ›å»ºï¼‰
                fingerprint_group = TableGroup.query.filter_by(
                    schema_fingerprint=new_fingerprint,
                    column_count=len(cleaned_columns)
                ).first()
                
                if fingerprint_group:
                    print(f"[ç³»ç»Ÿ] å‘ç°ç›¸åŒæŒ‡çº¹çš„åˆ†ç»„: {fingerprint_group.group_name}ï¼Œç›´æ¥ä½¿ç”¨")
                    return fingerprint_group
            
                # ç”Ÿæˆç®€æ´çš„ä¸­æ–‡åˆ†ç»„å
                existing_count = 1
                while True:
                    chinese_num = cls._number_to_chinese(existing_count)
                    group_name = f"åˆå¹¶è¡¨{chinese_num}"
                    
                    # æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å·²å­˜åœ¨
                    existing_group = TableGroup.query.filter_by(group_name=group_name).first()
                    if not existing_group:
                        break
                    existing_count += 1
                    
                    # é¿å…æ— é™å¾ªç¯
                    if existing_count > 100:
                        group_name = f"åˆå¹¶è¡¨{existing_count}_{int(time.time())}"
                        break
                
                # åˆ›å»ºåˆ†ç»„
                group = TableGroup(
                    group_name=group_name,
                    description=f"åŸºäºæ–‡ä»¶ {filename} åˆ›å»ºçš„è¡¨æ ¼åˆ†ç»„",
                    schema_fingerprint=new_fingerprint,
                    column_count=len(cleaned_columns),
                    confidence_score=1.0  # æ–°åˆ›å»ºçš„åˆ†ç»„ç½®ä¿¡åº¦ä¸º100%
                )
                
                session.add(group)
                session.flush()  # è·å–group.id
                
                # åˆ›å»ºschema
                for i, col_name in enumerate(cleaned_columns):
                    schema = TableSchema(
                        column_name=col_name,
                        column_type='text',
                        column_order=i,
                        is_active=True,
                        table_group_id=group.id
                    )
                    session.add(schema)
                
                print(f"[ç³»ç»Ÿ] åˆ›å»ºæ–°è¡¨æ ¼åˆ†ç»„: {group_name}")
                return group
    
    @staticmethod
    def create_column_mappings(group, original_columns, target_columns, filename, similarity_score):
        """åˆ›å»ºåˆ—åæ˜ å°„å…³ç³»"""
        print(f"[ç³»ç»Ÿ] åˆ›å»ºåˆ—æ˜ å°„å…³ç³»ï¼Œç›¸ä¼¼åº¦: {similarity_score:.2f}")
        
        for orig_col, target_col in zip(original_columns, target_columns):
            # è®¡ç®—å•åˆ—ç›¸ä¼¼åº¦
            col_similarity = SequenceMatcher(None, orig_col.lower().strip(), target_col.lower().strip()).ratio()
            
            mapping = ColumnMapping(
                table_group_id=group.id,
                original_column=orig_col,
                mapped_column=target_col,
                source_file=filename,
                similarity_score=col_similarity,
                is_confirmed=col_similarity >= UniversalExcelProcessor.HIGH_SIMILARITY_THRESHOLD
            )
            db.session.add(mapping)
        
        db.session.commit()
    
    @classmethod
    def process_excel_file_with_grouping(cls, file_path, filename):
        """å¤„ç†Excelæ–‡ä»¶å¹¶è¿›è¡Œæ™ºèƒ½åˆ†ç»„ - å¢å¼ºç‰ˆæœ¬ï¼ŒåŒ…å«æ–‡ä»¶å¤§å°æ£€æŸ¥å’Œé”™è¯¯å¤„ç†"""
        print(f"[ç³»ç»Ÿ] å¼€å§‹å¤„ç†Excelæ–‡ä»¶è¿›è¡Œæ™ºèƒ½åˆ†ç»„: {filename}")
        
        try:
            # 1. æ–‡ä»¶å¤§å°æ£€æŸ¥
            import os
            file_size = os.path.getsize(file_path)
            max_size = 100 * 1024 * 1024  # 100MB é™åˆ¶
            
            if file_size > max_size:
                return False, f"æ–‡ä»¶è¿‡å¤§ï¼ˆ{file_size/1024/1024:.1f}MBï¼‰ï¼Œæœ€å¤§æ”¯æŒ100MB", 0, None
            
            print(f"[ç³»ç»Ÿ] æ–‡ä»¶å¤§å°: {file_size/1024/1024:.2f}MB")
            
            # 2. å°è¯•è¯»å–æ–‡ä»¶ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
            try:
                df_raw = pd.read_excel(file_path, engine='openpyxl', header=None)
            except Exception as e1:
                try:
                    # å°è¯•ä½¿ç”¨xlrdå¼•æ“ï¼ˆé€‚ç”¨äº.xlsæ–‡ä»¶ï¼‰
                    df_raw = pd.read_excel(file_path, engine='xlrd', header=None)
                    print("[ç³»ç»Ÿ] ä½¿ç”¨xlrdå¼•æ“è¯»å–æ–‡ä»¶")
                except Exception as e2:
                    return False, f"æ— æ³•è¯»å–Excelæ–‡ä»¶: {str(e1)}", 0, None
            
            if df_raw.empty:
                return False, "æ–‡ä»¶ä¸ºç©º", 0, None
                
            # 3. åŸºæœ¬æ•°æ®éªŒè¯
            total_rows, total_cols = df_raw.shape
            print(f"[ç³»ç»Ÿ] æ–‡ä»¶ç»´åº¦: {total_rows} è¡Œ x {total_cols} åˆ—")
            
            if total_rows > 50000:
                return False, f"æ•°æ®è¡Œæ•°è¿‡å¤šï¼ˆ{total_rows}è¡Œï¼‰ï¼Œæœ€å¤§æ”¯æŒ50000è¡Œ", 0, None
            
            if total_cols > 200:
                return False, f"åˆ—æ•°è¿‡å¤šï¼ˆ{total_cols}åˆ—ï¼‰ï¼Œæœ€å¤§æ”¯æŒ200åˆ—", 0, None
            
            # æ£€æµ‹è¡¨å¤´
            header_row = cls.detect_header_row(df_raw)
            df = pd.read_excel(file_path, engine='openpyxl', header=header_row)
            
            # æ¸…ç†æ•°æ®
            df = df.dropna(how='all').dropna(axis=1, how='all')
            if df.empty:
                return False, "æ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆæ•°æ®", 0, None
            
            # æ¸…ç†åˆ—å
            original_columns = cls.clean_column_names(df.columns)
            df.columns = original_columns
            
            print(f"[ç³»ç»Ÿ] æ£€æµ‹åˆ°çš„åˆ—å: {original_columns}")
            
            # æ›´æ–°è¿›åº¦
            cls._update_progress('åˆ†ç»„å¤„ç†', 30, 'æ­£åœ¨æŸ¥æ‰¾åŒ¹é…çš„è¡¨æ ¼åˆ†ç»„...')
            
            # å¼ºåŒ–ç‰ˆåˆ†ç»„å¤„ç†é€»è¾‘
            print("[ç³»ç»Ÿ] ==================== å¼€å§‹åˆ†ç»„å¤„ç† ====================")
            
            # æŸ¥æ‰¾åŒ¹é…çš„è¡¨æ ¼åˆ†ç»„
            matching_group, similarity = cls.find_matching_table_group(original_columns)
            
            if matching_group is None:
                print("[ç³»ç»Ÿ] æœªæ‰¾åˆ°åŒ¹é…åˆ†ç»„ï¼Œåˆ›å»ºæ–°åˆ†ç»„")
                # åˆ›å»ºæ–°åˆ†ç»„ï¼ˆå†…éƒ¨å·²æœ‰å¤šé‡æ£€æŸ¥ï¼‰
                group = cls.create_table_group(original_columns, filename)
                target_columns = cls.clean_column_names(original_columns)  # ç¡®ä¿ä½¿ç”¨æ¸…ç†åçš„åˆ—å
                print(f"[ç³»ç»Ÿ] âœ… æˆåŠŸåˆ›å»º/è·å–åˆ†ç»„: {group.group_name} (ID: {group.id})")
            else:
                print(f"[ç³»ç»Ÿ] æ‰¾åˆ°åŒ¹é…åˆ†ç»„: {matching_group.group_name} (ID: {matching_group.id}), ç›¸ä¼¼åº¦: {similarity:.3f}")
                group = matching_group
                
                # è·å–ç›®æ ‡åˆ—ç»“æ„
                target_schemas = TableSchema.query.filter_by(
                    table_group_id=group.id, 
                    is_active=True
                ).order_by(TableSchema.column_order).all()
                target_columns = [schema.column_name for schema in target_schemas]
                
                print(f"[ç³»ç»Ÿ] ç›®æ ‡åˆ—ç»“æ„: {target_columns}")
                
                # æ›´æ–°åˆ†ç»„çš„ç½®ä¿¡åº¦ï¼ˆåŸºäºå†å²å¹³å‡ç›¸ä¼¼åº¦ï¼‰
                current_confidence = group.confidence_score or 1.0
                current_file_count = len(group.data_records) + 1  # åŒ…æ‹¬å³å°†æ·»åŠ çš„æ–‡ä»¶
                
                # è®¡ç®—æ–°çš„ç½®ä¿¡åº¦ï¼šåŠ æƒå¹³å‡
                new_confidence = ((current_confidence * (current_file_count - 1)) + similarity) / current_file_count
                group.confidence_score = new_confidence
                
                print(f"[ç³»ç»Ÿ] æ›´æ–°ç½®ä¿¡åº¦: {current_confidence:.3f} -> {new_confidence:.3f}")
                
                # å¦‚æœä¸æ˜¯å®Œå…¨åŒ¹é…ï¼Œåˆ›å»ºæ˜ å°„å…³ç³»
                if similarity < cls.EXACT_MATCH_THRESHOLD:
                    print(f"[ç³»ç»Ÿ] ç›¸ä¼¼åº¦ {similarity:.3f} < 1.0ï¼Œåˆ›å»ºåˆ—æ˜ å°„å…³ç³»")
                    cls.create_column_mappings(
                        group, original_columns, target_columns, filename, similarity
                    )
                else:
                    print("[ç³»ç»Ÿ] å®Œå…¨åŒ¹é…ï¼Œæ— éœ€åˆ—æ˜ å°„")
                
                print(f"[ç³»ç»Ÿ] âœ… ä½¿ç”¨ç°æœ‰åˆ†ç»„: {group.group_name}")
            
            print("[ç³»ç»Ÿ] ==================== åˆ†ç»„å¤„ç†å®Œæˆ ====================")
            
            # éªŒè¯åˆ†ç»„çŠ¶æ€
            if not group or not group.id:
                raise Exception("åˆ†ç»„åˆ›å»ºå¤±è´¥ï¼Œgroupä¸ºç©ºæˆ–æ— æ•ˆ")
                
            print(f"[ç³»ç»Ÿ] æœ€ç»ˆä½¿ç”¨åˆ†ç»„: {group.group_name} (ID: {group.id})")
            
            # æ›´æ–°è¿›åº¦
            cls._update_progress('æ•°æ®å¯¼å…¥', 60, 'æ­£åœ¨å¯¼å…¥æ•°æ®...')
            
            # å¯¼å…¥æ•°æ® - åˆ†æ‰¹å¤„ç†ä¼˜åŒ–å†…å­˜ä½¿ç”¨
            imported_count = 0
            batch_size = 1000  # æ¯æ‰¹å¤„ç†1000æ¡è®°å½•
            batch_data = []
            
            total_rows = len(df)
            print(f"[ç³»ç»Ÿ] å¼€å§‹åˆ†æ‰¹å¯¼å…¥æ•°æ®ï¼Œæ€»è¡Œæ•°: {total_rows}ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
            
            for index, row in df.iterrows():
                if row.isna().all():
                    continue
                
                # åˆ›å»ºæ•°æ®å­—å…¸ï¼Œä½¿ç”¨ç›®æ ‡åˆ—å
                row_dict = {}
                for orig_col, target_col in zip(original_columns, target_columns):
                    value = row[orig_col] if orig_col in row.index else ""
                    if pd.notna(value):
                        row_dict[target_col] = str(value)
                    else:
                        row_dict[target_col] = ""
                
                # è·³è¿‡ç©ºè¡Œ
                if not any(v.strip() for v in row_dict.values() if v):
                    continue
                
                # åˆ›å»ºæ•°æ®è®°å½•
                table_data = TableData()
                table_data.source_file = filename
                table_data.table_group_id = group.id
                table_data.set_data(row_dict)
                
                batch_data.append(table_data)
                
                # è¾¾åˆ°æ‰¹æ¬¡å¤§å°æˆ–æœ€åä¸€è¡Œæ—¶æäº¤
                if len(batch_data) >= batch_size or index == total_rows - 1:
                    try:
                        db.session.add_all(batch_data)
                        db.session.commit()
                        imported_count += len(batch_data)
                        print(f"[ç³»ç»Ÿ] å·²å¯¼å…¥ {imported_count} æ¡æ•°æ®")
                        batch_data.clear()  # æ¸…ç©ºæ‰¹æ¬¡æ•°æ®
                    except Exception as e:
                        db.session.rollback()
                        print(f"[é”™è¯¯] æ‰¹æ¬¡å¯¼å…¥æ—¶å‡ºé”™: {str(e)}")
                        # å°è¯•é€æ¡å¯¼å…¥è¿™ä¸ªæ‰¹æ¬¡
                        for data in batch_data:
                            try:
                                db.session.add(data)
                                db.session.commit()
                                imported_count += 1
                            except Exception as e2:
                                db.session.rollback()
                                print(f"[é”™è¯¯] å¯¼å…¥ç¬¬ {imported_count+1} è¡Œæ•°æ®æ—¶å‡ºé”™: {str(e2)}")
                        batch_data.clear()
            
            print(f"[ç³»ç»Ÿ] æˆåŠŸå¯¼å…¥ {imported_count} æ¡æ•°æ®åˆ°åˆ†ç»„: {group.group_name}")
            
            # è®°å½•ä¸Šä¼ å†å²
            history = UploadHistory(
                filename=filename,
                rows_imported=imported_count,
                status='success'
            )
            history.set_columns(original_columns)
            db.session.add(history)
            db.session.commit()
            
            return True, f"å¯¼å…¥æˆåŠŸï¼Œåˆ†ç»„: {group.group_name}", imported_count, group.id
            
        except Exception as e:
            db.session.rollback()
            print(f"[é”™è¯¯] å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            return False, str(e), 0, None
    
    @staticmethod
    def _number_to_chinese(num):
        """å°†æ•°å­—è½¬æ¢ä¸ºä¸­æ–‡æ•°å­—"""
        chinese_numbers = ['', 'ä¸€', 'äºŒ', 'ä¸‰', 'å››', 'äº”', 'å…­', 'ä¸ƒ', 'å…«', 'ä¹', 'å']
        
        if num <= 0:
            return 'ä¸€'
        elif num <= 10:
            return chinese_numbers[num]
        elif num <= 19:
            return 'å' + chinese_numbers[num - 10]
        elif num <= 99:
            tens = num // 10
            ones = num % 10
            if ones == 0:
                return chinese_numbers[tens] + 'å'
            else:
                return chinese_numbers[tens] + 'å' + chinese_numbers[ones]
        else:
            # å¯¹äºæ›´å¤§çš„æ•°å­—ï¼Œç®€åŒ–å¤„ç†
            return str(num)
    
    @staticmethod
    def cleanup_duplicate_groups():
        """æ¸…ç†é‡å¤çš„è¡¨æ ¼åˆ†ç»„"""
        print("[ç³»ç»Ÿ] å¼€å§‹æ¸…ç†é‡å¤åˆ†ç»„...")
        
        try:
            # è·å–æ‰€æœ‰åˆ†ç»„ï¼ŒæŒ‰æŒ‡çº¹åˆ†ç»„
            all_groups = TableGroup.query.all()
            fingerprint_groups = {}
            
            for group in all_groups:
                fp = group.schema_fingerprint
                if fp not in fingerprint_groups:
                    fingerprint_groups[fp] = []
                fingerprint_groups[fp].append(group)
            
            cleaned_count = 0
            for fingerprint, groups in fingerprint_groups.items():
                if len(groups) > 1:
                    print(f"[ç³»ç»Ÿ] å‘ç°æŒ‡çº¹ {fingerprint[:10]}... æœ‰ {len(groups)} ä¸ªé‡å¤åˆ†ç»„")
                    
                    # é€‰æ‹©è®°å½•æ•°æœ€å¤šçš„ä½œä¸ºä¸»åˆ†ç»„
                    main_group = max(groups, key=lambda g: len(g.data_records))
                    
                    for group in groups:
                        if group.id != main_group.id:
                            print(f"[ç³»ç»Ÿ] åˆå¹¶åˆ†ç»„ {group.group_name} (ID: {group.id}) åˆ° {main_group.group_name} (ID: {main_group.id})")
                            
                            # è¿ç§»æ•°æ®
                            data_records = TableData.query.filter_by(table_group_id=group.id).all()
                            for record in data_records:
                                record.table_group_id = main_group.id
                            
                            # åˆ é™¤é‡å¤åˆ†ç»„
                            db.session.delete(group)
                            cleaned_count += 1
            
            if cleaned_count > 0:
                db.session.commit()
                print(f"[ç³»ç»Ÿ] âœ… æ¸…ç†å®Œæˆï¼Œåˆå¹¶äº† {cleaned_count} ä¸ªé‡å¤åˆ†ç»„")
            else:
                print("[ç³»ç»Ÿ] âœ… æ²¡æœ‰å‘ç°é‡å¤åˆ†ç»„")
                
            return cleaned_count
            
        except Exception as e:
            db.session.rollback()
            print(f"[é”™è¯¯] æ¸…ç†é‡å¤åˆ†ç»„æ—¶å‡ºé”™: {str(e)}")
            return 0
    
    @classmethod
    def validate_system_health(cls):
        """éªŒè¯ç³»ç»Ÿå¥åº·çŠ¶æ€å’ŒåŠŸèƒ½å®Œæ•´æ€§"""
        print("ğŸ” [ç³»ç»Ÿ] å¼€å§‹ç³»ç»Ÿå¥åº·æ£€æŸ¥...")
        
        issues = []
        
        try:
            # 1. æ£€æŸ¥æ•°æ®åº“è¿æ¥
            total_groups = TableGroup.query.count()
            total_data = TableData.query.count()
            print(f"âœ… [æ•°æ®åº“] è¿æ¥æ­£å¸¸ï¼Œå…± {total_groups} ä¸ªåˆ†ç»„ï¼Œ{total_data} æ¡æ•°æ®")
            
            # 2. æ£€æŸ¥é‡å¤åˆ†ç»„
            duplicate_count = cls.cleanup_duplicate_groups()
            if duplicate_count > 0:
                issues.append(f"å‘ç°å¹¶æ¸…ç†äº† {duplicate_count} ä¸ªé‡å¤åˆ†ç»„")
            
            # 3. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
            orphaned_data = db.session.query(TableData).outerjoin(
                TableGroup, TableData.table_group_id == TableGroup.id
            ).filter(TableGroup.id.is_(None)).count()
            
            if orphaned_data > 0:
                issues.append(f"å‘ç° {orphaned_data} æ¡å­¤å„¿æ•°æ®ï¼ˆæ— å¯¹åº”åˆ†ç»„ï¼‰")
            
            # 4. æ£€æŸ¥åˆ—ååˆ«ååŠŸèƒ½
            test_columns = ['ç¼–å·', 'ID', 'åç§°', 'name']
            normalized = [UniversalExcelProcessor._normalize_column_name(col) for col in test_columns]
            if normalized[0] == normalized[1] and normalized[2] == normalized[3]:
                print("âœ… [æ™ºèƒ½è¯†åˆ«] åˆ—ååˆ«ååŠŸèƒ½æ­£å¸¸")
            else:
                issues.append("åˆ—ååˆ«åè¯†åˆ«åŠŸèƒ½å¼‚å¸¸")
            
            # 5. æ£€æŸ¥ç›¸ä¼¼åº¦è®¡ç®—
            similarity = UniversalExcelProcessor.calculate_column_similarity(
                ['é¡¹ç›®ç¼–å·', 'é¡¹ç›®åç§°', 'ç”³è¯·éƒ¨é—¨'],
                ['ç¼–å·', 'åç§°', 'éƒ¨é—¨']
            )
            if similarity > 0.5:
                print(f"âœ… [ç›¸ä¼¼åº¦è®¡ç®—] åŠŸèƒ½æ­£å¸¸ï¼Œæµ‹è¯•ç›¸ä¼¼åº¦: {similarity:.3f}")
            else:
                issues.append(f"ç›¸ä¼¼åº¦è®¡ç®—å¯èƒ½å¼‚å¸¸ï¼Œæµ‹è¯•ç»“æœ: {similarity:.3f}")
            
            # 6. æ€§èƒ½æ£€æŸ¥
            import time
            start_time = time.time()
            
            # æ¨¡æ‹Ÿè®¡ç®—è¾ƒå¤§åˆ—è¡¨çš„ç›¸ä¼¼åº¦
            large_cols1 = [f"åˆ—{i}" for i in range(50)]
            large_cols2 = [f"col{i}" for i in range(50)]
            
            similarity = UniversalExcelProcessor.calculate_column_similarity(large_cols1, large_cols2)
            
            elapsed = time.time() - start_time
            if elapsed < 1.0:
                print(f"âœ… [æ€§èƒ½] 50åˆ—ç›¸ä¼¼åº¦è®¡ç®—è€—æ—¶ {elapsed:.3f}s")
            else:
                issues.append(f"æ€§èƒ½é—®é¢˜ï¼š50åˆ—ç›¸ä¼¼åº¦è®¡ç®—è€—æ—¶ {elapsed:.3f}s")
            
            # æ€»ç»“
            if not issues:
                print("ğŸ‰ [ç³»ç»Ÿ] å¥åº·æ£€æŸ¥é€šè¿‡ï¼Œæ‰€æœ‰åŠŸèƒ½æ­£å¸¸ï¼")
                return True, "ç³»ç»ŸçŠ¶æ€è‰¯å¥½"
            else:
                warning_msg = "å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š" + "ï¼›".join(issues)
                print(f"âš ï¸ [ç³»ç»Ÿ] {warning_msg}")
                return False, warning_msg
                
        except Exception as e:
            error_msg = f"å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}"
            print(f"âŒ [ç³»ç»Ÿ] {error_msg}")
            return False, error_msg